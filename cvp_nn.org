#+author: NÃ­colas Morazotti
#+title: Spherical Decoder via Neural Networks
#+options: ':t toc:nil author:t date:t title:t tex:t latex:t 
#+options: todo:nil d:nil stat:nil
#+exclude_tags: noexport
#+bibliography:/home/nicolas/Dropbox/USP/referencias.bib
#+startup: inlineimages content indent showstars hideblocks align
#+lang: br
#+todo: TODO(t) WAITING(w) IN-PROGRESS(p) | DONE(d) ABANDONED(b@)

* Preamble                                                         :noexport:
#+latex_class: article
#+latex_header: \usemintedstyle{emacs}
#+latex_header: \usepackage{geometry}
#+latex_header: \geometry{a4paper, left = 20mm, right = 20mm, top = 20mm, bottom=20mm}
#+latex_header: \usepackage[x11names]{xcolor}
#+latex_header: \setminted[python]{frame=lines, bgcolor=Snow2, framesep=1.5mm, linenos, firstnumber=last}
#+latex_header: \setminted[sh]{frame=lines, bgcolor=Snow2, framesep=1.5mm}
#+latex_header: \usepackage{MnSymbol}
#+latex_header: \usepackage[qm, braket]{qcircuit}
#+latex_header: \DeclareMathOperator{\tr}{Tr}
#+latex_header: \usepackage[AUTO]{babel}
#+latex_header: \newcommand{\multiprepareC}[2]{*+<1em,.9em>{\hphantom{#2}}\save[0,0].[#1,0];p\save !C  *{#2},p+RU+<0em,0em>;+LU+<+.8em,0em> **\dir{-}\restore\save +RD;+RU **\dir{-}\restore\save  +RD;+LD+<.8em,0em> **\dir{-} \restore\save +LD+<0em,.8em>;+LU-<0em,.8em> **\dir{-} \restore \POS  !UL*!UL{\cir<.9em>{u_r}};!DL*!DL{\cir<.9em>{l_u}}\restore}
#+latex_header: \newcommand{\prepareC}[1]{*{\xy*+=+<.5em>{\vphantom{#1\rule{0em}{.1em}}}*\cir{l^r};p\save*!L{#1} \restore\save+UC;+UC+<.5em,0em>*!L{\hphantom{#1}}+R **\dir{-} \restore\save+DC;+DC+<.5em,0em>*!L{\hphantom{#1}}+R **\dir{-} \restore\POS+UC+<.5em,0em>*!L{\hphantom{#1}}+R;+DC+<.5em,0em>*!L{\hphantom{#1}}+R **\dir{-} \endxy}}
#+latex_header: \DeclareMathOperator{\Tr}{Tr}
#+latex_header: \newcommand{\id}[1][]{\mathbb{I}_{#1}}
* TOC                                                          :toc:noexport:
- [[#introduction][Introduction]]
- [[#the-lattice-in-question][The lattice in question]]
- [[#four-qubits][Four qubits]]
- [[#data-generation][Data generation]]
- [[#network-implementation][Network implementation]]
- [[#bib][Bib]]
- [[#footnotes][Footnotes]]

* Introduction
Following cite:mohammadkarimi_deep_2019, we wish to treat the Closest
Vector in a lattice Problem (CVP) by the means of Spherical
decoding. Let me explain.

Suppose there is a lattice \(\Lambda\) (a discrete additive subgroup of, say,
\(\mathbb{R}^{n}\)), and a point \(\mathbf{x}\) such that \(\mathbf{x}\not\in
\Lambda\).
We wish to obtain the lattice vector \(\boldsymbol{\ell} \in \Lambda\) closest to
\(\mathbf{x}\), i.e.  minimizes the squared Euclidean
distance[fn:loss]. Notice that any point \(\boldsymbol{\ell}\in\Lambda\)
can be written as \(\boldsymbol{\ell} = G\mathbf{s}\), where
\(\mathbf{s}\) is a vector of its component in each direction given by
the basis matrix \(G\). Thus, any point \(\mathbf{x} = G\mathbf{s} +
\mathbf{w}\), where \(\mathbf{w}\) is some noise.

Spherical decoding tells us to search for a
hypersphere of radius \(r_i\) around \(\mathbf{x}\) such that there is
only one lattice point inside it. It follows that this point must be the
closest vector in the lattice.

However, the problem is not as easy as it sounds. We may choose some
\(r_i\) s.t. there is no point inside the hypersphere, but as we iterate,
\(r_{i+1}\) has more than one. To find the optimal \(r_i\), we make use of
the neural networks framework. We will implement this mainly in
=python=.

The DNN aims to provide \(q\) possible \(r_i\) s.t. we may be able to
find the closest vector in a lattice for \(y\). At first, we will
implement this by considering mean squared error as our loss function,
only to generalize it later by the complexity metric.

* The lattice in question
:PROPERTIES:
:CUSTOM_ID: sec:lattice
:END:

In our case, the lattice we wish to use is not a usual \(\mathbb{Z}^n\)
lattice with some basis \(G\), but it is a lattice generated by unitary transformations
diagonal in the computational basis, which requires us to sweat a bit.

As cite:nielsen_geometric_2005 points out, a Hamiltonian containing
terms only proportional to \(\mathbb{I}\) and \(\sigma_z\), called /diagonal in the
computational basis/, generates a unitary transformation that is able to
be approximated by a lattice, and this approximation gives us the least
quantum complexity of the transformation. Our lattice basis can be
chosen to be \(\{J_z\}_{z = 1,...,2n-1}\), \(J_z = 2\pi\left(\op{z}{z} - \op{0}{0}\right)\),
where \(n\) is the number of qubits of our system.

In our first approach, we wish to attack two qubits. Therefore, \(n=2
\implies \{J_1, J_2, J_3\}\).
We have four possible states:
\(\ket{0}\ket{0}, \ket{0}\ket{1}, \ket{1}\ket{0}, \ket{1}\ket{1}\),
which we can translate as
\begin{align}
  \ket{0} &\equiv \ket{0}\ket{0}\\
  \ket{1} &\equiv \ket{0}\ket{1}\\
  \ket{2} &\equiv \ket{1}\ket{0}\\
  \ket{3} &\equiv \ket{1}\ket{1}.
\end{align}
Clearly, we can define
\begin{align}
  \ket{0} &=
  \begin{pmatrix}
    1&0&0&0
  \end{pmatrix},\\
  \ket{1} &=
  \begin{pmatrix}
    0&1&0&0
  \end{pmatrix},\\
  \ket{2} &=
  \begin{pmatrix}
    0&0&1&0
  \end{pmatrix},\\
  \ket{3} &=
  \begin{pmatrix}
    0&0&0&1
  \end{pmatrix},
\end{align}
with \(\mathbb{I} = \sum_{i=0}^{3} \op{i}{i}\).

Once an all-identity term \(\mathbb{I}\otimes\mathbb{I}\) in the
Hamiltonian gives us no evolution in time, there is three possible terms
to consider: \(\mathbb{I}\otimes\sigma_z\),
\(\sigma_z\otimes\mathbb{I}\) and \(\sigma_z\otimes\sigma_z\). Thus, our
lattice is three-dimensional, and since \(z\) can not be zero, we may
define the basis of our system in terms of the non-zero-index diagonal term
of \(J_z\), which is a set of three three-dimensional vectors. We may
write then
\begin{align}
  \mathbf{J}_1 &=
  \begin{pmatrix}
    2\pi & 0 & 0
  \end{pmatrix},\\
    \mathbf{J}_2 &=
  \begin{pmatrix}
    0 &  2\pi & 0
  \end{pmatrix},\\
    \mathbf{J}_3 &=
  \begin{pmatrix}
    0 & 0 & 2\pi
  \end{pmatrix},
\end{align}
as the elements of our basis.

Now, to translate each possible element of our Hamiltonian, we must
analyze all tensor products:
\begin{align}
  \mathbb{I}\otimes\sigma_z&=
   \begin{bmatrix}
     1 & 0\\
     0 & 1
   \end{bmatrix}
         \otimes
  \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}\nonumber\\
  &=
    \begin{bmatrix}
      1 & 0 & 0 & 0\\
      0 & -1 & 0 & 0\\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & -1 \\
    \end{bmatrix},\\
  \sigma_z\otimes \mathbb{I}&=
   \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}
         \otimes
  \begin{bmatrix}
     1 & 0\\
     0 & 1
   \end{bmatrix}\nonumber\\
  &=
    \begin{bmatrix}
      1 & 0 & 0 & 0\\
      0 & 1 & 0 & 0\\
      0 & 0 & -1 & 0 \\
      0 & 0 & 0 & -1 \\
    \end{bmatrix},\\
  \sigma_z\otimes\sigma_z&=
   \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}
         \otimes
  \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}\nonumber\\
  &=
    \begin{bmatrix}
      1 & 0 & 0 & 0\\
      0 & -1 & 0 & 0\\
      0 & 0 & -1 & 0 \\
      0 & 0 & 0 & 1 \\
    \end{bmatrix}.
\end{align}
By analogy, we may infer them as the following vectors:
\begin{align}
  \mathbb{I}\otimes\sigma_z &= \frac{1}{2\pi}(-\mathbf{J}_1+\mathbf{J}_2-\mathbf{J}_3)\\
  \sigma_z\otimes\mathbb{I} &= \frac{1}{2\pi}(\mathbf{J}_1-\mathbf{J}_2-\mathbf{J}_3)\\
  \sigma_z\otimes\sigma_z &= \frac{1}{2\pi}(-\mathbf{J}_1-\mathbf{J}_2+\mathbf{J}_3).
\end{align}

Any time-dependent diagonal Hamiltonian for two qubits becomes
\begin{align}
  \mathbf{H}(t) = \frac{1}{2\pi}\big[ &H_{IZ}(t)(-\mathbf{J}_1+\mathbf{J}_2-\mathbf{J}_3)
      + H_{ZI}(t)(\mathbf{J}_1-\mathbf{J}_2-\mathbf{J}_3)
      + H_{ZZ}(t)(-\mathbf{J}_1-\mathbf{J}_2+\mathbf{J}_3)\big]\nonumber \\
   = \frac{1}{2\pi}\{&\mathbf{J}_1[H_{ZI}(t)-H_{IZ}(t)-H_{ZZ}(t)]\\
          + &\mathbf{J}_2[H_{IZ}(t)-H_{ZI}(t)-H_{ZZ}(t)]\\
          + &\mathbf{J}_3[H_{ZI}(t)-H_{IZ}(t)-H_{ZI}(t)]
          \}
\end{align}
in this lattice basis. 

* Four qubits
Our Hamiltonian will be composed of \(4\) qubits. If it is to be
diagonal, it can only be built using identities or \(\sigma_z\).

#+BEGIN_SRC jupyter-python :session py :exports both :results raw :eval never-export :async yes 
  from qutip import *
  import numpy as np


  basis = [qeye(2), sigmaz()]
  possible_combinations = [tensor(i,j,k,l) for i in basis for j in basis for k in basis for l in basis]

  for i in possible_combinations:
    print(np.diag(i[1:,1:]))
#+END_SRC

#+RESULTS:
#+begin_example
  [1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j
   1.+0.j 1.+0.j 1.+0.j 1.+0.j 1.+0.j]
  [-1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j
    1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j]
  [ 1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j
   -1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j]
  [-1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j
   -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j]
  [ 1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j
    1.+0.j  1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j]
  [-1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j
    1.+0.j -1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j]
  [ 1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j  1.+0.j  1.+0.j
   -1.+0.j -1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j]
  [-1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j
   -1.+0.j  1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j]
  [ 1.+0.j  1.+0.j  1.+0.j  1.+0.j  1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j
   -1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j]
  [-1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j
   -1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j]
  [ 1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j
    1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j]
  [-1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j
    1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j]
  [ 1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j
   -1.+0.j -1.+0.j  1.+0.j  1.+0.j  1.+0.j  1.+0.j]
  [-1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j
   -1.+0.j  1.+0.j  1.+0.j -1.+0.j  1.+0.j -1.+0.j]
  [ 1.+0.j -1.+0.j -1.+0.j -1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j
    1.+0.j  1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j]
  [-1.+0.j -1.+0.j  1.+0.j -1.+0.j  1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j
    1.+0.j -1.+0.j  1.+0.j -1.+0.j -1.+0.j  1.+0.j]
#+end_example

* Data generation
:PROPERTIES:
:CUSTOM_ID: sec:data_gen
:END:

To feed our neural network, we need data. As such, we will generate a
lot of points and compute the distance to each lattice node in a
separate python script. 

#+name: data:libs
#+BEGIN_SRC jupyter-python :session data :exports both :results raw :eval never :async yes :tangle data.py
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sys
#+END_SRC

#+name: data:lattice_matrix
#+BEGIN_SRC jupyter-python :session data :exports both :results raw :eval never :async yes :tangle data.py
  J = np.eye(1)
#+END_SRC

#+name: data:lattice_nodes
#+BEGIN_SRC jupyter-python :session data :exports both :results raw :eval never :async yes :tangle data.py
lattice_range = range(-10, 11)
global lattice
lattice = np.array([[i]@J
                    for i in lattice_range])
#+END_SRC

#+name: data:points_and_radii
#+BEGIN_SRC jupyter-python :session data :exports both :results raw :eval never :async yes :tangle data.py
def points_within(r, y):
    tof = np.linalg.norm(lattice-y, axis = 1) <= r 
    return lattice[tof]

def get_radius(lp, y):
    return np.linalg.norm(lp-y, axis = 1)
#+END_SRC

#+name: data:dummy_data
#+BEGIN_SRC jupyter-python :session data :exports both :results raw :eval never :async yes :tangle data.py
data = 20*np.random.random([100_000, 1])@J - 10
#+END_SRC

#+name: data:find_radii
#+BEGIN_SRC jupyter-python :session data :exports both :results raw :eval never :async yes :tangle data.py
  radii = []
  point = []
  for k,v in enumerate(data[:50_000]):
      point.append(v) 
      radii.append(np.sort(get_radius(lattice, v)))
      if k%5 == 0: print(f"{100*k/data.shape[0]}%")
  np.savez("/home/nicolas/points_radii_data_1.npz", points=point, radii=radii)
  point = []
  radii = []
  for k,v in enumerate(data[50_000:]):
      point.append(v) 
      radii.append(np.sort(get_radius(lattice, v)))
      if k%5 == 0: print(f"{100*k/data.shape[0]}%")
  np.savez("/home/nicolas/points_radii_data_2.npz", points=point, radii=radii)
#+END_SRC

#+name: run.sh
#+begin_src sh :shebang "#!/bin/bash" :tangle run.sh :comments org :eval never
  time python data.py
#+end_src


* Network implementation
:PROPERTIES:
:CUSTOM_ID: sec:nn_impl
:END:

We will implement a lattice as discussed by
cite:mohammadkarimi_deep_2019. To do so, let us begin importing all
necessary libraries.

#+name: libraries
#+BEGIN_SRC jupyter-python :session py :exports both :results none :eval never-export :async yes :tangle yes
  import matplotlib.pyplot as plt
  import numpy as np
  import pandas as pd

  from keras.models import Sequential
  from keras.layers import Dense, Lambda
  from keras import backend as K

  import tensorflow as tf

  from sklearn.cluster import KMeans
  from sklearn.utils import shuffle
#+END_SRC

The lattice basis can be written as follows
#+name: lattice_basis
#+BEGIN_SRC jupyter-python :session py :exports code :results none :eval never-export :async yes :tangle yes
  J = np.eye(3)
#+END_SRC
and we can write the whole lattice centers as 
#+name: lattice_centers
#+BEGIN_SRC jupyter-python :session py :exports code :results raw :eval never-export :async yes :tangle yes
  lattice_range = range(-10, 11)
  global lattice
  lattice = np.array([[i,j,k]@J
                      for i in lattice_range
                      for j in lattice_range
                      for k in lattice_range])
#+END_SRC

To list which points \(\lambda\in\Lambda\) are within a distance \(r\) to a
given point \(y \not\in \Lambda\), we check
\begin{align}
  |\lambda-y| \leq r.
\end{align}
#+BEGIN_SRC jupyter-python :session py :exports code :results raw :eval never-export :async yes :tangle yes
  def points_within(r, y):
      tof = np.linalg.norm(lattice-y, axis = 1) < r 
      return lattice[tof]

  def get_radius(lp, y):
      return np.linalg.norm(lp-y, axis = 1)
#+END_SRC

As we follow cite:mohammadkarimi_deep_2019, we notice that it is trained
by feeding the points as well as the basis matrix \(G\). Also, we need
to train it by comparing to the smaller \(q\) radii, obtained in
autoref:sec:data_gen.  The activation function of the hidden layer,
named CRelu, is a function that follows
\begin{align}
  CRelu(x) =
  \begin{cases}
    0,& x<0\\
    x,& 0\leq x<1\\
    1,& x\geq 1
  \end{cases}
\end{align}
which can also be written as
\begin{align}
  CRelu(x) = Relu(x) - Relu(x-1)
\end{align}
where
\begin{align}
  Relu(x) =
  \begin{cases}
    0,& x<0\\
    x,& x \geq 0 
  \end{cases}.
\end{align}
#+name: cReLU
#+BEGIN_SRC jupyter-python :session py :exports code :results raw :eval never-export :async yes :tangle yes
  def _crelu(x):
   return K.relu(x) - K.relu(x-1)
#+END_SRC

#+RESULTS: cReLU

The neural network is built by feeding it with the point in question,
\(y\), and the basis matrix, \(J\).

#+name: model_building
#+BEGIN_SRC jupyter-python :session py :exports both :results raw  :eval never-export :async yes :tangle yes :var q = 10
  model = Sequential()
  model.add(Dense(128, input_dim = 220, activation = _crelu))
  model.add(Dense(q, input_dim = 128))
  model.compile(loss='mean_squared_error', optimizer='adam')
  model.summary()
#+END_SRC

#+RESULTS: model_building
#+begin_example
  Model: "sequential_1"
  _________________________________________________________________
  Layer (type)                 Output Shape              Param #   
  =================================================================
  dense (Dense)                (None, 128)               28288     
  _________________________________________________________________
  dense_1 (Dense)              (None, 10)                1290      
  =================================================================
  Total params: 29,578
  Trainable params: 29,578
  Non-trainable params: 0
  _________________________________________________________________
#+end_example

* Bib                                                                :ignore:
#+LaTeX: \newpage
#+LaTeX: \bibliographystyle{unsrt}
#+LaTeX: \bibliography{/home/nicolas/Dropbox/USP/referencias.bib}
* Footnotes
 [fn:loss:We may minimize other distances as well.] 

* Local vars                                                       :noexport:
# Local Variables:
# eval: (auto-fill-mode)
# eval: (flyspell-mode)
# eval: (ispell-change-dictionary "english")
# eval: (org-toggle-pretty-entities)
# eval: (undo-tree-mode)
# eval: (setq org-src-window-setup 'current-window)
# eval: (add-to-list 'org-latex-packages-alist '("" "minted"))
# eval: (setq org-latex-pdf-process '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f" "bibtex %b" "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f" "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
# eval: (setq org-confirm-babel-evaluate nil)
# eval: (LaTeX-math-mode)
# eval: (setq org-format-latex-options '(:foreground default :background default :scale 1.6 :html-foreground "Black" :html-background "Transparent" :html-scale 1.0 :matchers ("begin" "$1" "$" "$$" "\\(" "\\[")))
# eval: (setq org-latex-prefer-user-labels t)
# End:

