#+author: NÃ­colas Morazotti
#+title: Spherical Decoder via Neural Networks
#+options: ':t toc:nil author:t date:t title:t tex:t latex:t 
#+options: todo:nil d:nil stat:nil
#+exclude_tags: noexport
#+bibliography:/home/nicolas/Dropbox/USP/referencias.bib
#+startup: inlineimages content indent showstars hideblocks align
#+lang: br
#+todo: TODO(t) WAITING(w) IN-PROGRESS(p) | DONE(d) ABANDONED(b@)

* Preamble                                                         :noexport:
#+latex_class: article
#+latex_header: \usemintedstyle{emacs}
#+latex_header: \usepackage{geometry}
#+latex_header: \geometry{a4paper, left = 20mm, right = 20mm, top = 20mm, bottom=20mm}
#+latex_header: \usepackage[x11names]{xcolor}
#+latex_header: \setminted[python]{frame=lines, bgcolor=Snow2, framesep=1.5mm, linenos, firstnumber=last}
#+latex_header: \setminted[sh]{frame=lines, bgcolor=Snow2, framesep=1.5mm}
#+latex_header: \usepackage{MnSymbol}
#+latex_header: \usepackage[qm, braket]{qcircuit}
#+latex_header: \DeclareMathOperator{\tr}{Tr}
#+latex_header: \usepackage[AUTO]{babel}
#+latex_header: \newcommand{\multiprepareC}[2]{*+<1em,.9em>{\hphantom{#2}}\save[0,0].[#1,0];p\save !C  *{#2},p+RU+<0em,0em>;+LU+<+.8em,0em> **\dir{-}\restore\save +RD;+RU **\dir{-}\restore\save  +RD;+LD+<.8em,0em> **\dir{-} \restore\save +LD+<0em,.8em>;+LU-<0em,.8em> **\dir{-} \restore \POS  !UL*!UL{\cir<.9em>{u_r}};!DL*!DL{\cir<.9em>{l_u}}\restore}
#+latex_header: \newcommand{\prepareC}[1]{*{\xy*+=+<.5em>{\vphantom{#1\rule{0em}{.1em}}}*\cir{l^r};p\save*!L{#1} \restore\save+UC;+UC+<.5em,0em>*!L{\hphantom{#1}}+R **\dir{-} \restore\save+DC;+DC+<.5em,0em>*!L{\hphantom{#1}}+R **\dir{-} \restore\POS+UC+<.5em,0em>*!L{\hphantom{#1}}+R;+DC+<.5em,0em>*!L{\hphantom{#1}}+R **\dir{-} \endxy}}
#+latex_header: \DeclareMathOperator{\Tr}{Tr}
#+latex_header: \newcommand{\id}[1][]{\mathbb{I}_{#1}}
* TOC                                                          :toc:noexport:
- [[#introduction][Introduction]]
- [[#the-lattice-in-question][The lattice in question]]
- [[#data-generation][Data generation]]
- [[#lattice-implementation][Lattice implementation]]
- [[#bib][Bib]]
- [[#footnotes][Footnotes]]

* Introduction
Following cite:mohammadkarimi_deep_2019, we wish to treat the Closest
Vector in a lattice Problem (CVP) by the means of Spherical
decoding. Let me explain.

Suppose there is a lattice \(\Lambda\) (a discrete additive subgroup of, say,
\(\mathbb{R}^{n}\)), and a point \(\mathbf{x}\) such that \(\mathbf{x}\not\in
\Lambda\).
We wish to obtain the lattice vector \(\boldsymbol{\ell} \in \Lambda\) closest to
\(\mathbf{x}\), i.e.  minimizes the squared Euclidean
distance[fn:loss]. Spherical decoding tells us to search for a
hypersphere of radius \(r_i\) around \(\mathbf{x}\) such that there is
only one lattice point inside it. It follows that this point must be the
closest vector in the lattice.

However, the problem is not as easy as it sounds. We may choose some
\(r_i\) s.t. there is no point inside the hypersphere, but as we iterate,
\(r_{i+1}\) has more than one. To find the optimal \(r_i\), we make use of
the neural networks framework. We will implement this mainly in
=python=.

The DNN aims to provide \(q\) possible \(r_i\) s.t. we may be able to
find the closest vector in a lattice for \(y\). At first, we will
implement this by considering binary cross entropy as our loss function,
only to generalize it later by the complexity metric.


* The lattice in question
In our case, the lattice we wish to use is not a usual \(\mathbb{Z}^n\)
lattice with some basis \(G\), but it is a lattice generated by unitary transformations
diagonal in the computational basis, which requires us to sweat a bit.

As cite:nielsen_geometric_2005 points out, a Hamiltonian containing
terms only proportional to \(\mathbb{I}\) and \(\sigma_z\), called /diagonal in the
computational basis/, generates a unitary transformation that is able to
be approximated by a lattice, and this approximation gives us the least
quantum complexity of the transformation. Our lattice basis can be
chosen to be \(\{J_z\}_{z = 1,...,2n-1}\), \(J_z = 2\pi\left(\op{z}{z} - \op{0}{0}\right)\),
where \(n\) is the number of qubits of our system.

In our first approach, we wish to attack two qubits. Therefore, \(n=2
\implies \{J_1, J_2, J_3\}\).
We have four possible states:
\(\ket{0}\ket{0}, \ket{0}\ket{1}, \ket{1}\ket{0}, \ket{1}\ket{1}\),
which we can translate as
\begin{align}
  \ket{0} &\equiv \ket{0}\ket{0}\\
  \ket{1} &\equiv \ket{0}\ket{1}\\
  \ket{2} &\equiv \ket{1}\ket{0}\\
  \ket{3} &\equiv \ket{1}\ket{1}.
\end{align}
Clearly, we can define
\begin{align}
  \ket{0} &=
  \begin{pmatrix}
    1&0&0&0
  \end{pmatrix},\\
  \ket{1} &=
  \begin{pmatrix}
    0&1&0&0
  \end{pmatrix},\\
  \ket{2} &=
  \begin{pmatrix}
    0&0&1&0
  \end{pmatrix},\\
  \ket{3} &=
  \begin{pmatrix}
    0&0&0&1
  \end{pmatrix},
\end{align}
with \(\mathbb{I} = \sum_{i=0}^{3} \op{i}{i}\).

Once an all-identity term \(\mathbb{I}\otimes\mathbb{I}\) in the
Hamiltonian gives us no evolution in time, there is three possible terms
to consider: \(\mathbb{I}\otimes\sigma_z\),
\(\sigma_z\otimes\mathbb{I}\) and \(\sigma_z\otimes\sigma_z\). Thus, our
lattice is three-dimensional, and since \(z\) can not be zero, we may
define the basis of our system in terms of the non-zero-index diagonal term
of \(J_z\), which is a set of three three-dimensional vectors. We may
write then
\begin{align}
  \mathbf{J}_1 &=
  \begin{pmatrix}
    2\pi & 0 & 0
  \end{pmatrix},\\
    \mathbf{J}_2 &=
  \begin{pmatrix}
    0 &  2\pi & 0
  \end{pmatrix},\\
    \mathbf{J}_3 &=
  \begin{pmatrix}
    0 & 0 & 2\pi
  \end{pmatrix},
\end{align}
as the elements of our basis.

Now, to translate each possible element of our Hamiltonian, we must
analyze all tensor products:
\begin{align}
  \mathbb{I}\otimes\sigma_z&=
   \begin{bmatrix}
     1 & 0\\
     0 & 1
   \end{bmatrix}
         \otimes
  \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}\nonumber\\
  &=
    \begin{bmatrix}
      1 & 0 & 0 & 0\\
      0 & -1 & 0 & 0\\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & -1 \\
    \end{bmatrix},\\
  \sigma_z\otimes \mathbb{I}&=
   \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}
         \otimes
  \begin{bmatrix}
     1 & 0\\
     0 & 1
   \end{bmatrix}\nonumber\\
  &=
    \begin{bmatrix}
      1 & 0 & 0 & 0\\
      0 & 1 & 0 & 0\\
      0 & 0 & -1 & 0 \\
      0 & 0 & 0 & -1 \\
    \end{bmatrix},\\
  \sigma_z\otimes\sigma_z&=
   \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}
         \otimes
  \begin{bmatrix}
     1 & 0\\
     0 & -1
   \end{bmatrix}\nonumber\\
  &=
    \begin{bmatrix}
      1 & 0 & 0 & 0\\
      0 & -1 & 0 & 0\\
      0 & 0 & -1 & 0 \\
      0 & 0 & 0 & 1 \\
    \end{bmatrix}.
\end{align}
By analogy, we may infer them as the following vectors:
\begin{align}
  \mathbb{I}\otimes\sigma_z &= \frac{1}{2\pi}(-\mathbf{J}_1+\mathbf{J}_2-\mathbf{J}_3)\\
  \sigma_z\otimes\mathbb{I} &= \frac{1}{2\pi}(\mathbf{J}_1-\mathbf{J}_2-\mathbf{J}_3)\\
  \sigma_z\otimes\sigma_z &= \frac{1}{2\pi}(-\mathbf{J}_1-\mathbf{J}_2+\mathbf{J}_3).
\end{align}

Any time-dependent diagonal Hamiltonian for two qubits becomes
\begin{align}
  \mathbf{H}(t) = \frac{1}{2\pi}\big[ &H_{IZ}(t)(-\mathbf{J}_1+\mathbf{J}_2-\mathbf{J}_3)
      + H_{ZI}(t)(\mathbf{J}_1-\mathbf{J}_2-\mathbf{J}_3)
      + H_{ZZ}(t)(-\mathbf{J}_1-\mathbf{J}_2+\mathbf{J}_3)\big]\nonumber \\
   = \frac{1}{2\pi}\{&\mathbf{J}_1[H_{ZI}(t)-H_{IZ}(t)-H_{ZZ}(t)]\\
          + &\mathbf{J}_2[H_{IZ}(t)-H_{ZI}(t)-H_{ZZ}(t)]\\
          + &\mathbf{J}_3[H_{ZI}(t)-H_{IZ}(t)-H_{ZI}(t)]
          \}
\end{align}
in this lattice basis. 

* Data generation
To feed our NN

* Lattice implementation
We will implement a lattice as discussed by
cite:mohammadkarimi_deep_2019. To do so, let us begin importing all
necessary libraries.

#+name: libraries
#+BEGIN_SRC jupyter-python :session py :exports both :results none :eval never-export :async yes :tangle yes
  import matplotlib.pyplot as plt
  import numpy as np
  import pandas as pd

  from keras.models import Sequential
  from keras.layers import Dense, Lambda
  from keras import backend as K

  import tensorflow as tf

  from sklearn.cluster import KMeans
  from sklearn.utils import shuffle
#+END_SRC

The lattice basis can be written as follows
#+name: lattice_basis
#+BEGIN_SRC jupyter-python :session py :exports code :results none :eval never-export :async yes :tangle yes
  J = np.eye(3)
#+END_SRC
and we can write the whole lattice centers as 
#+name: lattice_centers
#+BEGIN_SRC jupyter-python :session py :exports code :results raw :eval never-export :async yes :tangle yes
  lattice_range = range(-10, 11)
  global lattice
  lattice = np.array([[i,j,k]@J
                      for i in lattice_range
                      for j in lattice_range
                      for k in lattice_range])
#+END_SRC

#+RESULTS: lattice_centers

To list which points \(\lambda\in\Lambda\) are within a distance \(r\) to a
given point \(y \not\in \Lambda\), we check
\begin{align}
  |\lambda-y| \leq r.
\end{align}

#+BEGIN_SRC jupyter-python :session py :exports code :results raw :eval never-export :async yes :tangle yes
  def points_within(r, y):
      tof = np.linalg.norm(lattice-y, axis = 1) < r 
      return lattice[tof]

  def get_radius(lp, y):
      return np.linalg.norm(lp-y, axis = 1)
#+END_SRC

#+RESULTS:

To train our lattice, we need data. To obtain it, we will use the KMeans
method as discussed [[file:~/Dropbox/USP/Doutorado/quali/quali.pdf][here]]. We will generate \(10^6\) points to train our
lattice, and find our radius using =get_radius= function.
#+name: dummy_data
#+BEGIN_SRC jupyter-python :session py :exports code :results none :eval never-export :async yes :tangle yes
  data = 20*np.random.random([1_000_000, 3])@J - 10
#+END_SRC

We will use a previous method of finding the closest lattice point as a
bridge to gather data to train the neural network: first, we will use
the KMeans method to classify those \(10^6\) points, and use only the
correctly characterized as data for our neural network. The correctly
characterized data has =df["R2"] = 1=.
#+BEGIN_SRC jupyter-python :session py :exports code :results none :eval never :async yes :tangle yes
  sample = data[:100_000]
  n_clusters = lattice.shape[0]
  kmeans = KMeans(n_clusters = n_clusters, init = lattice, random_state = 190794).fit(sample)
  labels = kmeans.predict(data)
#+END_SRC

#+BEGIN_SRC jupyter-python :session py :exports code :results none :eval never :async yes :tangle yes
  cols = {}
  cols["r_x"] = data[:,0]
  cols["r_y"] = data[:,1]
  cols["r_z"] = data[:,2]
  cols["nearest l_x"] = lattice[labels][:,0]
  cols["nearest l_y"] = lattice[labels][:,1]
  cols["nearest l_z"] = lattice[labels][:,2]
  cols["radius"] = get_radius(data, lattice[labels])

  aux = []
  for x,y,z,lx,ly,lz in zip(*[cols[i] for i in cols.keys()]):
      aux.append(r2([lx,ly,lz],[np.round(x), np.round(y), np.round(z)]))
  cols["R2"] = aux

  df = pd.DataFrame(cols)
#+END_SRC

#+BEGIN_SRC jupyter-python :session py :exports results :results output :eval never-export :async yes :tangle yes
  df = pd.read_pickle("dummy_data.pkl")
  print(df[df["R2"] == 1])
#+END_SRC

#+RESULTS:
#+begin_example
               r_x       r_y       r_z  nearest l_x  nearest l_y  nearest l_z  \
  0       3.886191 -6.101527  6.933873          4.0         -6.0          7.0   
  1       0.245916 -6.694230  8.985356          0.0         -7.0          9.0   
  2      -1.636898 -2.999371 -4.994185         -2.0         -3.0         -5.0   
  3      -2.049347  8.003784  8.841865         -2.0          8.0          9.0   
  6      -6.890361 -1.719359 -9.363550         -7.0         -2.0         -9.0   
  ...          ...       ...       ...          ...          ...          ...   
  999992  7.328897  1.057908  2.114701          7.0          1.0          2.0   
  999993  4.358878  8.647659  4.944413          4.0          9.0          5.0   
  999994  8.852299  4.801257  5.064726          9.0          5.0          5.0   
  999996  5.714428 -1.843551 -8.164321          6.0         -2.0         -8.0   
  999998 -4.070636  3.265408 -3.756355         -4.0          3.0         -4.0   

           R2    radius  
  0       1.0  0.166231  
  1       1.0  0.392664  
  2       1.0  0.363149  
  3       1.0  0.165699  
  6       1.0  0.472174  
  ...     ...       ...  
  999992  1.0  0.353105  
  999993  1.0  0.505992  
  999994  1.0  0.255938  
  999996  1.0  0.364731  
  999998  1.0  0.367143  

  [617902 rows x 8 columns]
#+end_example

#+BEGIN_SRC jupyter-python :session py :exports both :results raw :eval never-export :async yes 
  points_within(0.790930, [-7.569034, 7.608160, -0.461634])
#+END_SRC

#+RESULTS:
: array([[-8.,  8.,  0.]])


The activation function of the hidden layer, named CRelu, is a function
that follows
\begin{align}
  CRelu(x) =
  \begin{cases}
    0,& x<0\\
    x,& 0\leq x<1\\
    1,& x\geq 1
  \end{cases}
\end{align}
which can also be written as
\begin{align}
  CRelu(x) = Relu(x) - Relu(x-1)
\end{align}
where
\begin{align}
  Relu(x) =
  \begin{cases}
    0,& x<0\\
    x,& x \geq 0 
  \end{cases}.
\end{align}
#+name: cReLU
#+BEGIN_SRC jupyter-python :session py :exports code :results raw :eval never-export :async yes :tangle yes
  def _crelu(x):
   return K.relu(x) - K.relu(x-1)
#+END_SRC

#+RESULTS: cReLU

The neural network is built by feeding it with the point in question,
\(y\), and the basis matrix, \(J\).

#+name: model_building
#+BEGIN_SRC jupyter-python :session py :exports both :results raw  :eval never-export :async yes :tangle yes :var q = 10
  model = Sequential()
  model.add(Dense(128, input_dim = 220, activation = _crelu))
  model.add(Dense(q, input_dim = 128))
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  model.summary()
#+END_SRC

#+RESULTS: model_building
#+begin_example
  Model: "sequential_1"
  _________________________________________________________________
  Layer (type)                 Output Shape              Param #   
  =================================================================
  dense (Dense)                (None, 128)               28288     
  _________________________________________________________________
  dense_1 (Dense)              (None, 10)                1290      
  =================================================================
  Total params: 29,578
  Trainable params: 29,578
  Non-trainable params: 0
  _________________________________________________________________
#+end_example

* Bib                                                                :ignore:
#+LaTeX: \newpage
#+LaTeX: \bibliographystyle{unsrt}
#+LaTeX: \bibliography{/home/nicolas/Dropbox/USP/referencias.bib}
* Footnotes
 [fn:loss:We may minimize other distances as well.] 

* Local vars                                                       :noexport:
# Local Variables:
# eval: (auto-fill-mode)
# eval: (flyspell-mode)
# eval: (ispell-change-dictionary "english")
# eval: (org-toggle-pretty-entities)
# eval: (undo-tree-mode)
# eval: (setq org-src-window-setup 'current-window)
# eval: (add-to-list 'org-latex-packages-alist '("" "minted"))
# eval: (setq org-latex-pdf-process '("xelatex -shell-escape -interaction nonstopmode -output-directory %o %f" "bibtex %b" "xelatex -shell-escape -interaction nonstopmode -output-directory %o %f" "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
# eval: (setq org-confirm-babel-evaluate nil)
# eval: (LaTeX-math-mode)
# eval: (setq org-format-latex-options '(:foreground default :background default :scale 1.6 :html-foreground "Black" :html-background "Transparent" :html-scale 1.0 :matchers ("begin" "$1" "$" "$$" "\\(" "\\[")))
# eval: (setq org-latex-prefer-user-labels t)
# End:

